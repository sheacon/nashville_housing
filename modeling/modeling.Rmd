---
title: "Modeling"
author: "Shea Conaway"
output: github_document
---

```{r, message=FALSE}
# packages
```

# Data

```{r}
# data
df <- read.csv('../data/2_cleaned/cleaned_data.csv')
```

```{r}
# one-hot encode categoricals

# home type
# default single family
# encode condo or townhouse
df$condo <- ifelse(df$home_type == 'CONDO', 1, 0)
df$townhouse <- ifelse(df$home_type == 'TOWNHOUSE', 1, 0)

# neighborhood
# default neighborhood 1 (Green Hills / Forest Hills / Belle Meade)
neighborhoods <- unique(df$neighborhood)
num_hoods <- length(neighborhoods) # 24
# loop encoding
for(i in 2:num_hoods) {
  new <- ifelse(df$neighborhood == neighborhoods[i], 1, 0)
  df[ , ncol(df) + 1] <- new
  colnames(df)[ncol(df)] <- paste0('neighborhood_', i) 
  }
```

```{r}
# time variables

# basic time variable
# captures inflation in nashville market
df$time <- as.numeric(as.Date(df$date_sold) - as.Date('2021-11-11'))

# days between
df$days_since_previous_sale <- as.numeric(as.Date(df$date_sold) - as.Date(df$date_sold_previous))

```

```{r}
# subset to desired variables
df <- subset(df,select = -c(zpid
                   ,price_sqft
                   ,home_type
                   ,date_sold
                   ,date_listed
                   #,days_on_market
                   ,date_sold_previous
                   #,age
                   ,year_built
                   ,description
                   ,photo_count
                   #,longitude
                   #,latitude
                   ,neighborhood
                   ,address_state
                   ,address_city
                   ,address_zipcode
                   ,address_street
                   ,parcel_id
                   ,url
                   ,favorite_count
                   ,page_view_count
                   ,home_status))

```

```{r}
# feature transformations and imputations

# imputation
df$days_on_market[is.na(df$days_on_market)] <- mean(df$days_on_market, na.rm = TRUE)
df$days_since_previous_sale[is.na(df$days_since_previous_sale)] <- 0 # never sold before
df$previous_price[is.na(df$previous_price)] <- df$price[is.na(df$previous_price)] # fill na's
df$previous_price[df$previous_price > df$price] <- df$price[df$previous_price > df$price] # price should increase
df$lot_size[df$lot_size < df$living_area] <- df$living_area[df$lot_size < df$living_area] # lot size at least sqft

# log transformations
df$bedrooms <- log(df$bedrooms)
df$bathrooms <- log(df$bathrooms)
df$living_area <- log(df$living_area)
df$lot_size <- log(df$lot_size)

# sqrt transformations
df$days_on_market <- sqrt(df$days_on_market)
df$age <- sqrt(df$age)
df$previous_price <- sqrt(df$previous_price)
```

# Model

## Linear Regression

Linear regression models are well-understood and easily explained. They serve as a good baseline model in a regression task to gut-check more sophisticated approaches.

Linear regression assumptions
- Linearity: the relationship between the feature set and the target variable is linear
- Homoscedasticity: the variance of the residuals is constant
- Independence: all observations are independent of one another
- Normality: the distribution of the target variable is normal

The target variable distribution is right skewed, as expected with home prices. The log transformation does a decent job of normalizing, which is more appropriate for a linear model. Log transformations are also applied to the skewed features.

```{r}
summary(df$price)
```


```{r}
d = density(df$price)
plot(d, main = 'price')
polygon(d, col='gray')

d_log = density(log(df$price))
plot(d_log, main = 'Log Price')
polygon(d_log, col='gray')
```

We split our data in train/validate/test sets. The train dataset is used for fitting our models. Validate is used for comparing models. While train and validate will be used multiple times, our final test set is used only once to estimate real-world performance.

```{r}
# train/validate/test split
library(splitTools)
set.seed(20221217)

# 80/10/10
inds <- splitTools::partition(df$price, p = c(train = 0.6, valid = 0.1, test = 0.1))
str(inds)

train <- df[inds$train, ]
valid <- df[inds$valid, ]
test <- df[inds$test, ]

```


```{r}
# linear model training
model_lm = lm(log(price) ~ ., data=train)
summary(model_lm)
```

It is good practice to check for multicollinearity in the predictors for a linear model, as the presence of relationships between the predictors can make coefficients and their p-values unreliable. Here, we have some severe variance inflaction factor scores for variables having to do with location.

Given that our primary interest is in prediction performance, rather than the independent relationship between each predictor and the price target, we can leave this issue unaddressed.

```{r}
# check for multicollinearity in our predictors
library(car) # variance inflation factor
vif <- car::vif(model_lm) # variance inflation factor
vif[vif > 5] # severe
```

In our diagnostic plots, we're assessing the assumptions we've made in a linear model.
- In the Residuals vs Fitted plot, we're seeing a stable goodness of fit for most of the fitted value range, with some issues at the tails. Also, the spread of residuals shifts at the upper end, representing some homoskedasticity.
- In the Normal QQ-plot, we're seeing strong evidence of non-linear characteristics not captured by our model based on the non-normality of the residuals.
- The Cook's distance plot checks for observations that have high influence on the model fit. Here there are three observations worthy of investigation.

```{r, fig.width = 10}
# linear regression diagnostic plots
dev.new(width=100, height=50, unit="px")
plot(model_lm, which = c(1,2,4))
```


```{r}
# linear regression prediction and root mean squared error
pred_lm <- predict(model_lm, newdata = valid)
rmse_lm <- sqrt(sum((exp(pred_lm) - valid$price)^2)/length(valid$price))
rmse_lm

```

In this plot of actual vs predicted, the model performs better for lower cost housing.

Over $700,000 the model appears to underestimate prices. Only 8% of the houses are in this range. For more expensive houses, there are likely characteristics we don't have in our data that capture some of their value. Think luxury features like hardwood floors, expensive lighting fixtures, ensuite bathrooms, etc.

```{r}
# plot
plot(valid$price, exp(pred_lm))
abline(coef = c(0, 1), c = 'red')
sum(df$price > 700000)/length(df$price)
```

## XGBoost

```{r}
# xgboost package
library(xgboost) 
```


XGBoost belongs to a class of models popular throughout many industries because of superior performance on a variety problems. Its benefits include capturing non-linear relationships, detecing complex interactions, and robustness to outliers and other data issues.

An XGBoost model consists of many weak classifiers trained iteratively to reduce residuals, also known as boosting. This decision-tree based ensemble algorithm uses the gradient boosting framework, which allows for flexibility in loss function selection.


```{r}
# additional xgboost data formatting

# train
train_x = data.matrix(train[, -1])
train_y = train[,1]
# test
valid_x = data.matrix(valid[, -1])
valid_y = valid[,1]
# final format for xgboost
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_valid = xgb.DMatrix(data = valid_x, label = valid_y)
```

```{r}
# xgboost training
model_xgb = xgb.train(data = xgb_train, max.depth = 3, nrounds = 350)
```

```{r}
# xgb prediction and root mean squared error
pred_xgb <- predict(model_xgb, newdata = valid_x)
rmse_xgb <- sqrt(sum((pred_xgb - valid$price)^2)/length(valid$price))
rmse_xgb
```

XGBoost results in a 25% reduction in RMSE compared to the linear fit.

Although not as pronounced as the linear fit, the XGBoost model is still underestimating more expensive homes.

```{r}
# plot
plot(valid$price, pred_xgb, main = 'XGBoost Predicted vs Actuals', xlab = 'Actuals', ylab = 'Predicted')
abline(coef = c(0, 1), c = 'red')
1 - rmse_xgb/rmse_lm # performance comparison
```
```{r}
# prep analysis dataframe
xgb_analysis <- data.frame(1:length(valid$price),valid$price, pred_xgb)
xgb_analysis$ae <- abs(xgb_analysis$valid.price - xgb_analysis$pred_xgb) # absolute error
xgb_analysis$ae_p <- xgb_analysis$ae/xgb_analysis$valid.price
```

The mean absolute error for predicted price is \$36k or 10% of true home price. The median figures are \$22k and 7%.

```{r}
summary(xgb_analysis$ae)
summary(xgb_analysis$ae_p)
```



## SAINT (Work in Progress)
### Self-Attention and Intersample Attention Transformer
- A hybrid deep learning approach to solving tabular data problems
- SAINT performs attention over both rows and columns and includes an enhanced embedding method

# Test

Once we've settled on our modeling decisions, we can train our test model on all the non-test data and assess real-world performance on the test set. I'm not ready to do this yet.

```{r}
# test model training

# model_test = model(price ~ ., data=(train + valid))
```


```{r}
# final model prediction and root mean squared error

# pred_test <- predict(model_test, newdata = test)
# rmse_test <- sqrt(sum(pred_test - test$price)^2)/length(test$price))
# rmse_test
```

# Final Model

When it's all said and done, we can train our final model on all the data we have. Then we're ready to use our model to price some houses!

```{r}
# final model training

# model_final = model(price ~ ., data=df)
```








