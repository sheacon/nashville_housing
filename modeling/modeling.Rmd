---
title: "Modeling"
author: "Shea Conaway"
output: github_document
---

```{r, message=FALSE}
# packages
library(xgboost) #for fitting the xgboost model
library(caret) # data prep and model fitting
```

# Data

```{r}
# data
df <- read.csv('../data/2_cleaned/cleaned_data.csv')
```

```{r}
# one-hot encode categoricals

# home type
# default single family
# encode condo or townhouse
df$condo <- ifelse(df$home_type == 'CONDO', 1, 0)
df$townhouse <- ifelse(df$home_type == 'TOWNHOUSE', 1, 0)

# neighborhood
# default neighborhood 1 (Green Hills / Forest Hills / Belle Meade)
neighborhoods <- unique(df$neighborhood)
num_hoods <- length(neighborhoods) # 24
# loop enconding
for(i in 2:num_hoods) {
  new <- ifelse(df$neighborhood == neighborhoods[i], 1, 0)
  df[ , ncol(df) + 1] <- new
  colnames(df)[ncol(df)] <- paste0('neighborhood_', i) 
  }
```

```{r}
# subset to desired variables
df <- subset(df,select = -c(zpid
                   ,price_sqft
                   ,home_type
                   ,date_sold
                   ,date_listed
                   ,days_on_market
                   ,date_sold_previous
                   ,age
                   ,year_built
                   ,description
                   ,photo_count
                   #,longitude
                   #,latitude
                   ,neighborhood
                   ,address_state
                   ,address_city
                   ,address_zipcode
                   ,address_street
                   ,parcel_id
                   ,url
                   ,favorite_count
                   ,page_view_count
                   ,home_status))

```

```{r}
# a little additional lot size cleaning
df <- df[!df$lot_size == 0,]
df <- df[!df$lot_size == 1,]
# lot size should be at least as large as living area
df$lot_size[df$lot_size < df$living_area] <- df$living_area[df$lot_size < df$living_area]
```

# Model

## Linear Regression

Linear regression models are well-understood and easily explained. They serve as a good baseline model in a regression task to gut-check more sophisticated approaches.

The target variable distribution is right skewed, as expected with home prices. The log transformation does a decent job of normalizing, which is more appropriate for a linear model. Log transformations are also applied to the skewed features.

```{r}
d = density(df$price)
plot(d, main = 'price')
polygon(d, col='gray')

d_log = density(log(df$price))
plot(d_log, main = 'price')
polygon(d_log, col='gray')
```

```{r}
# feature log transformations
df$bedrooms <- log(df$bedrooms)
df$bathrooms <- log(df$bathrooms)
df$living_area <- log(df$living_area)
df$lot_size <- log(df$lot_size)
```

```{r}
# train/test split
set.seed(20221217)

# 80/20
parts = caret::createDataPartition(df$price, p = .8, list = F)
train = df[parts, ]
test = df[-parts, ]

dim(train)
dim(test)
```


```{r}
# linear model training
model_lm = lm(log(price) ~ ., data=train)
summary(model_lm)
```

```{r}
# linear regression prediction and error
pred_lm <- predict(model_lm, newdata = test)
rmse_lm <- sqrt(sum((exp(pred_lm) - test$price)^2)/length(test$price))
rmse_lm

```

```{r}
# plot
plot(test$price, exp(pred_lm))
abline(coef = c(0, 1), c = 'red')
```

## XGBoost

XGBoost belongs to a class of models popular throughout many industries because of its performance on a variety problems. It is a decision-tree-based ensemble algorithm that uses the gradient boosting framework. Most boosting algorithms consist of iteratively learning weak classifiers to reduce residuals. A gradient-boosted trees model generalizes this method by allowing optimization of an arbitrary differentiable loss function.

```{r}
# additional xgboost data formatting

# train
train_x = data.matrix(train[, -1])
train_y = train[,1]
# test
test_x = data.matrix(test[, -1])
test_y = test[,1]
# final format for xgboost
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
```

```{r}
# xgboost training
model_xgb = xgb.train(data = xgb_train, max.depth = 3, nrounds = 350)
```

```{r}
# xgb prediction and error
pred_xgb <- predict(model_xgb, newdata = test_x)
rmse_xgb <- caret::RMSE(test_y, pred_xgb)
rmse_xgb
```

```{r}
# plot
plot(test$price, pred_xgb)
abline(coef = c(0, 1), c = 'red')
```


# Comparison

XGBoost results in a 9% reduction in RMSE.

```{r}
# performance comparison
1 - rmse_xgb/rmse_lm
```


